[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Full BioHobbies\n\n\nA little bit more about me… before attending the University of California, Santa Barbara (UCSB), I worked for Bank of Hawai’i, the largest local bank in the state. My career started with the bank when I was brought on as a student assistant. I was lucky to have managers that believed in my personal and professional development, check out my short testimony on the Bank of Hawaii IT Career page.\nMy managers nominated me for the bank’s internship program and their management training program. My manager wanted to continued to invest in my professional development and asked me consider pursuing a master’s in business administration (MBA). However I decided to go back to school to pursue something related to my passion for sustainability.\nMy passion for sustainability comes from being born and raised in Hawai’i. It was easy to fall in love with the environment spending most of my time growing up outdoors. But part of growing up in Hawai’i is also seeing how limited our natural resources are and how that affects us. In high school I had the opportunity to volunteer with Aloha Harvest. I saw how many people suffered from food insecurity and how an organization turned a problem, like food waste, into a solution to help to communities get fed.\nIdeally, I would love my next career to be one where I can combine my IT finance experience with my passion in sustainability and newly gained data science skills. I hope to work in green technology, corporate sustainability, food waste and/or environmental, social, and corporate governance (ESG), but am open to other opportunities.\n\n\n\nTrying new restaurants with family and friends\nChecking out local coffee shops and farmer’s markets\nSpending time at the beach\nTraveling\n\n“It seems that the more places I see and experience, the bigger I realize the world to be. The more I become aware of, the more I realize how relatively little I know of it, how many places I have still to go, how much more there is to learn.” - Anthony Bourdain\nA map of my travels so far :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ruth Enriquez",
    "section": "",
    "text": "Aloooooha!\n\n\nBioHighlighted ExperienceEducation\n\n\nHi there, my name is Ruth Enriquez! I recently earned my Master’s of Environmental Data Science degree (MEDS) from the University of California, Santa Barbara. I am an aspiring data scientist, and would love the opportunity to combine my passions for information technology, finance, data, and sustainability in my next career. I hope to work in green technology, corporate sustainability, food waste and/or environmental, social, and corporate governance (ESG), but am open to other opportunities.\n\n\nMaster’s Capstone Project 2023: Evaluating Carbon Emissions from Electric Arc Furnace Steel Plants in the United States Role: Project Manager Links: Analysis & Tableau Dashboard\nDescription: Working with Global Energy Monitor our capstone group brought transparency in how much Scope 2 emissions were being produced by Electric Arc Furnace Steel Plants. We used four different data sets to paint us a complete picture of Scope 2 emissions. After standardizing our data using industry standards we created an interactive Tableau dashboard of our findings.\n\n\n\n\n\n\n\n\nGraduate\n\nMaster’s of Environmental Data Science, 2023\nUniversity of California, Santa Barbara, Bren School for Environmental Science & Management\n\nUndergraduate\n\nBachelor of Business Administration, Finance, 2017\nBachelor of Business Administration, Management Information System, 2017\nUniversity of Hawai’i at Mānoa, Shidler College of Business"
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html",
    "title": "Food Waste Analysis",
    "section": "",
    "text": "I wanted to look at the relationship between food waste production and household income, specifically do households with higher income produce more food waste? I wanted to investigate this relationship because 10% of American’s suffer from food insecurity, yet every year we waste 40% of all food. Of the food waste produced in America 31% of it is produced at the wholesale & retail level. I wanted to see if there was an opportunity to divert edible food waste/excess to individuals instead of landfills. I think this is an important topic to investigate because everyone should have access to food. This topic is also important because reducing the amount of food waste in landfills can help reduce the amount of methane produced and help to mitigate climate change. Doing a quick Google search, I see there are others wondering about the effects of socioeconomic status on food waste production. There is a research paper, Community social capital and status: The social dilemma of food waste, that look at the negative relationship food waste levels have related to local levels of social capital in Italy. However, I couldn’t find other’s asking a similar question for America. Though there seems to be a growing interest in food waste data and statistics. I think there is still a gap in the data collection of food waste produced in America therefore there isn’t a existing evidence on the question of do households with higher income produce more food waste in America."
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html#data",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html#data",
    "title": "Food Waste Analysis",
    "section": "Data",
    "text": "Data\nI got my food waste data from the Environmental Protection Agency (EPA) & my household income data from the United States Census Bureau. For this project, I decided to shrink my scope & to focus on food waste produced at the wholesale & retail level & household income in California counties.\nI quickly learned that most companies do not rigorously track food waste, so all the numbers I used from the EPA data are estimates. From the data the EPA was able to collect they ran it through an algorithm that produced low and high estimates of food wasted for the year. There was little transparency on how the EPA calculated the low and high food waste estimates. With this limitation, I decided to apply my analysis to both estimates provided to see if there was a relationship.\n\n\nShow the code\nlibrary(here)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sjPlot)\nlibrary(rempsyc)\nlibrary(broom)\n\n# Set your filepaths here! Or, set this up as an .Rproj if you'd like.\nrootdir <- (\"C:/Users/ruthe/Documents/F22/EDS222/statsFoodWaste\")\ndatadir <- file.path(rootdir,\"data\")\n#setwd(file.path(rootdir,\"homework\",\"assignment-02-ruthe808\"))\n\n#loading food waste data on wholesale and retail \nfoodRaw <- read_excel(file.path(datadir, \"foodWaste\", file = \"Food_Wholesale_Retail.xlsx\"), sheet = \"Data\") |> \n  clean_names() |> \n  mutate(excessfood_tonyear_lowest = as.numeric(excessfood_tonyear_lowest), excessfood_tonyear_highest = as.numeric(excessfood_tonyear_highest)) |> \n  filter(is.na(excessfood_tonyear_lowest) == FALSE, is.na(excessfood_tonyear_highest) == FALSE, county != \"NULL\")\n\n#loading in CA county & income census data\nCensus <- read_excel(file.path(datadir, file = \"census2020.xlsx\"), sheet = \"Data2\") |> \n  clean_names()\n\nfood <- foodRaw |> \n  select(\"county\", \"state\", \"excessfood_tonyear_lowest\", \"excessfood_tonyear_highest\") |> \n  filter(state == \"CA\") |> \n  group_by(county) |> \n  summarise(Mean_Food_Waste_Low = mean(excessfood_tonyear_lowest), Mean_Food_Waste_High = mean(excessfood_tonyear_highest))\n\n#joining the food and census tables together\nfoodCensus <- left_join(Census, food, by = \"county\") |> \n  select(\"county\",\n         \"mean_household_income_dollars\",\n         \"Mean_Food_Waste_Low\",\n         \"Mean_Food_Waste_High\")\n\n#loading in county by region data\nregion <- read_excel(file.path(datadir, file = \"countyRegion.xlsx\"))\n\n\n#Making a region/county data frame with food waste data\n#Use later for getting count to calculate pvalue\nfoodRegion<- left_join(foodCensus, region, by = \"county\")\n\n#Looking at regional trend for LOW estimate\nfoodRegionLow<- left_join(foodCensus, region, by = \"county\") |> \n  group_by(Region) |> \n  summarise(Mean_Low = mean(Mean_Food_Waste_Low),\n            SD_Low = sd(Mean_Food_Waste_Low))\n\n#Looking at regional trend for HIGH estimate\nfoodRegionHigh<- left_join(foodCensus, region, by = \"county\") |> \n  group_by(Region) |> \n  summarise(Mean_High = mean(Mean_Food_Waste_High),\n            SD_High = sd(Mean_Food_Waste_High))"
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html#exploring-my-data",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html#exploring-my-data",
    "title": "Food Waste Analysis",
    "section": "Exploring my Data",
    "text": "Exploring my Data\n\nChecking my data distribution\nI decided to look at how my data was distributed by plotting it on a histogram. After some data wrangling this was the best, I could normalize my data.\n\n\nShow the code\n#checking my data distribution\n#incomeHist <- hist(foodCensus$mean_household_income_dollars)\n\nincomeHist <- ggplot(data = foodCensus)+\n  geom_histogram(aes(x =mean_household_income_dollars), fill = \"cyan4\", bins = 8)+\n  theme_classic()+\n  labs(title = \"Histogram of Mean Household Income in Dollars\",\n       x = \"Mean Household Income ($)\")\n  \n#lowHist <- hist(foodCensus$Mean_Food_Waste_Low)\nlowHist <- ggplot(data = foodCensus)+\n  geom_histogram(aes(x = Mean_Food_Waste_Low), fill = \"cyan4\", bins = 8)+\n  theme_classic()+\n  labs(title = \"Histogram of Mean Food Waste Low Estimate\",\n       x = \"Mean Food Waste (Tons)\")\n\n#highHist <- hist(foodCensus$Mean_Food_Waste_High)\nhighHist <- ggplot(data = foodCensus)+\n  geom_histogram(aes(x = Mean_Food_Waste_High), fill = \"cyan4\", bins = 8)+\n  theme_classic()+\n  labs(title = \"Histogram of Mean Food Waste High Estimate\",\n       x = \"Mean Food Waste (Tons)\")\n\nincomeHist\n\n\n\n\n\nShow the code\nlowHist\n\n\n\n\n\nShow the code\nhighHist\n\n\n\n\n\n\n\nIs this there an obvious relationship?\nOut of curiosity I also plotted my data in a scatter plot to see if there was an obvious relationship between food waste production and household income. For the low food waste estimate there didn’t seem to be a relationship, while the high food waste estimate seemed to have a positive relationship.\n\n\nShow the code\n#initially checking if there is a relationship between average household income and average food waste produced\nlowWaste <- ggplot(data = foodCensus,\n                            aes(x = mean_household_income_dollars,\n                                y = Mean_Food_Waste_Low)) +\n  geom_point(size = 3) +\n  theme_classic() +\n  labs(x = \"Mean Food Waste Low (tons)\",\n       y = \"Mean Household Income ($)\")\n\nlowWaste\n\n\n\n\n\nShow the code\nhighWaste <- ggplot(data = foodCensus,\n                            aes(x = mean_household_income_dollars,\n                                y = Mean_Food_Waste_High)) +\n  geom_point(size = 3) +\n  theme_classic() +\n  labs(x = \"Mean Food Waste High (tons)\",\n       y = \"Mean Household Income ($)\")\n\n\nhighWaste"
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html#analysis-plan",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html#analysis-plan",
    "title": "Food Waste Analysis",
    "section": "Analysis Plan",
    "text": "Analysis Plan\nAfter exploring my data, I wanted to see if there really a relationship between the food waste was estimates and household incomes. To investigate this relationship further I decided that I wanted to run a linear regression model and do hypothesis testing. Part of the reason why I decided to do these analyses is because of the limitations I had with the data available. With the limited amount of data, I had I wasn’t able to do other analyses."
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html#running-linear-regressions",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html#running-linear-regressions",
    "title": "Food Waste Analysis",
    "section": "Running Linear Regressions",
    "text": "Running Linear Regressions\n\nVisualizing Linear Regressions\n\nLow Food Waste Estimate\n\\[\\text{food waste low}_i = \\beta_0 + \\beta_1 \\text{mean household income}_i + \\varepsilon_i\\]\n\n\nShow the code\n#Creating a linear regression on LOW food waste estimate\nregLow <-lm(Mean_Food_Waste_Low ~ mean_household_income_dollars, data =foodCensus)\ntab_model(regLow)\n\n\n\n\n\n \nMean Food Waste Low\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n3.54\n2.44 – 4.64\n<0.001\n\n\nmean household incomedollars\n-0.00\n-0.00 – 0.00\n0.733\n\n\nObservations\n58\n\n\nR2 / R2 adjusted\n0.002 / -0.016\n\n\n\n\n\n\n\n\nShow the code\n#Plotting linear regression for LOW food waste estimate\nlowWastePlot <- ggplot(data = foodCensus,\n                       aes(x = mean_household_income_dollars,\n                           y = Mean_Food_Waste_Low)) +\n  labs(x = \"Mean Household Income ($)\",\n      y = \"Food Waste Low Estimate (tons)\") +\n  geom_point(alpha = 0.5, size = 3) + \n  geom_smooth(method ='lm', \n              formula = y~x, \n              color =\"lightcoral\", \n              se = F, size = 1.5) +\n  theme_classic()\n\nlowWastePlot\n\n\n\n\n\nWhen I ran the linear regression for the low food waste estimate, it showed that as household income increased there was a slight decrease in the amount of food waste produced. However, with such a high p-value, the mapped relationship is not significance. I do not have confidence to accept this relationship.\n\n\nHigh Food Waste Estimate\n\\[\\text{food waste high}_i = \\beta_0 + \\beta_1 \\text{mean household income}_i + \\varepsilon_i\\]\n\n\nShow the code\n#Creating a linear regression on HIGH food waste estimate\nregHigh <-lm(Mean_Food_Waste_High ~ mean_household_income_dollars, data =foodCensus)\ntab_model(regHigh)\n\n\n\n\n\n \nMean Food Waste High\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n122.45\n118.26 – 126.63\n<0.001\n\n\nmean household incomedollars\n0.00\n0.00 – 0.00\n0.001\n\n\nObservations\n58\n\n\nR2 / R2 adjusted\n0.193 / 0.178\n\n\n\n\n\n\n\n\nShow the code\n#Plotting linear regression for HIGH food waste estimate\nhighWastePlot <- ggplot(data = foodCensus,\n                        aes(x = mean_household_income_dollars,\n                            y = Mean_Food_Waste_High)) +\n  labs(x = \"Mean Household Income ($)\",\n      y = \"Food Waste High Estimate (tons)\") +\n  geom_point(alpha = 0.5, size = 3) + \n  geom_smooth(method ='lm',\n              formula = y~x,\n              color =\"lightcoral\",\n              se = F,\n              size = 1.5) +\n  theme_classic()\n\nhighWastePlot\n\n\n\n\n\nWhen I ran a linear regression on the high food waste estimates, the data said the opposite. As household income increased the increase in production of food waste was more significant for this data. In this model the p-value was much better, and the mapped relationship is significant. I would have more confidence accepting this relationship if both estimates were producing similar results with similar p-values."
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html#hypothesis-testing",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html#hypothesis-testing",
    "title": "Food Waste Analysis",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nI wanted to continue my analysis by doing a t.test. I chose two different regions in California with varying average household incomes. I chose to the Northern San Joaquin Valley region with an average household income of about 75,000 dollars & the San Francisco Bay Area region with an average household income of about 156,000 dollars. I wanted to see how the relationship between the two regions within the two different estimates.\n\nLow\n\\[H_{0}: \\mu_{SoSJ} - \\mu_{SfBay} = 0\\] \\[H_{A}: \\mu_{SoSJ} - \\mu_{SfBay} \\neq 0\\]\n\n\nShow the code\n#Computing point estimate of your parameter of interest\n#finding the mean inputs for the point estimate calculation\nmuSoSjL <- (foodRegionLow$Mean_Low[foodRegionLow$Region==\"Southern San Joaquin Valley\"])\nmuSfBayL <- (foodRegionLow$Mean_Low[foodRegionLow$Region==\"San Francisco Bay Area\"])\n\n#calculating point estimate\npointEstL = round(as.numeric(muSoSjL - muSfBayL) , 3)\n\n#calculating standard error\ncountSoSj = foodRegion |> \n  filter(Region == \"Southern San Joaquin Valley\") |> \n  count()\ncountSfBay = foodRegion |> \n  filter(Region == \"San Francisco Bay Area\") |> \n  count()\n\n#calling out the standard deviation\nsdSoSjL <- (foodRegionLow$SD_Low[foodRegionLow$Region==\"Southern San Joaquin Valley\"])\nsdSfBayL <- (foodRegionLow$SD_Low[foodRegionLow$Region==\"San Francisco Bay Area\"])\n\n#calculating standard error\nseFoodL = round(as.numeric(sqrt(sdSoSjL^2/countSoSj + sdSfBayL^2/countSfBay)),3)\n\n#calculating test statistic/zscore\nzScoreL = round(((pointEstL - 0)/seFoodL),3)\n\n#calculating our p-value using pt\npvalL <- pt(zScoreL, 26, lower.tail = FALSE)\n\n#Doing a t.test to check my work\nt1 <- t.test(foodRegion$Mean_Food_Waste_Low[foodRegion$Region==\"Southern San Joaquin Valley\"], foodRegion$Mean_Food_Waste_Low[foodRegion$Region==\"San Francisco Bay Area\"])\n\nstats.table <- tidy(t1, conf.int = TRUE)\nnice_table(stats.table, broom = \"t.test\")\n\n\n\nMethodAlternativeMean 1Mean 2M1 - M2tdfp95% CIWelch Two Sample t-testtwo.sided3.733.180.560.969.00.361[-0.75, 1.87]\n\n\nAfter running the t.test on the low food waste estimates I observed that there was a slight difference in the means. But with our p-value at 0.361 I would fail to reject the null hypothesis. For the low food waste estimates, I would fail to reject the null hypothesis. And for the high food waste estimates I would reject the null hypothesis.\n\n\nHigh\n\\[H_{0}: \\mu_{SoSJ} - \\mu_{SfBay} = 0\\] \\[H_{A}: \\mu_{SoSJ} - \\mu_{SfBay} \\neq 0\\]\n\n\nShow the code\n#Computing point estimate of your parameter of interest\n#finding the mean inputs for the point estimate calculation\nmuSoSj <- (foodRegionHigh$Mean_High[foodRegionHigh$Region==\"Southern San Joaquin Valley\"])\nmuSfBay <- (foodRegionHigh$Mean_High[foodRegionHigh$Region==\"San Francisco Bay Area\"])\n\n#calculating point estimate\npointEst = round(as.numeric(muSoSj - muSfBay) , 3)\n\n#calling out the standard deviation\nsdSoSj <- (foodRegionHigh$SD_High[foodRegionHigh$Region==\"Southern San Joaquin Valley\"])\nsdSfBay <- (foodRegionHigh$SD_High[foodRegionHigh$Region==\"San Francisco Bay Area\"])\n\n#calculating standard error\nseFood = round(as.numeric(sqrt(sdSoSj^2/countSoSj + sdSfBay^2/countSfBay)),3)\n\n#calculating test statistic/zscore\nzScore = round(((pointEst - 0)/seFood),3)\n\n#calculating our p-value using pt\npval <- pt(zScore, 26, lower.tail = FALSE)\n\n#Doing a t.test to check my work\nt2 <- t.test(foodRegion$Mean_Food_Waste_High[foodRegion$Region==\"Southern San Joaquin Valley\"], foodRegion$Mean_Food_Waste_High[foodRegion$Region==\"San Francisco Bay Area\"])\n\nstats.table <- tidy(t2, conf.int = TRUE)\nnice_table(stats.table, broom = \"t.test\")\n\n\n\nMethodAlternativeMean 1Mean 2M1 - M2tdfp95% CIWelch Two Sample t-testtwo.sided129.15132.87-3.72-2.958.69.017[-6.59, -0.85]\n\n\nRunning the t.test on the high food waste estimates I observed that there was a greater difference in the means, but we had a p-value of 0.015. Once again, the p-value for the high food waste estimate was better than the low p-value."
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html#future-work",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html#future-work",
    "title": "Food Waste Analysis",
    "section": "Future Work",
    "text": "Future Work\nOverall, after my analyses I do not think I can confidently answer my question. The first reason why I cannot answer my question is the data itself. The data is based off estimates ran from an algorithm that had no transparency to how they were calculated. Second, the two estimates tell different stories and there isn’t enough information for me to know if the difference in story is due to how each estimate was calculated. The third reason I cannot answer my question is the p-values for the low estimate were not good. If I were to do this over again, I would hope that there would be actual food waste data and not estimates. I think having actual food waste data would help significantly. I think also having data for more than a year would also be helpful. Having more data would show more trends. It would also give me an opportunity to do a time series analysis."
  },
  {
    "objectID": "posts/2022-12-09-stats-final-project/food_waste_stats.html#references",
    "href": "posts/2022-12-09-stats-final-project/food_waste_stats.html#references",
    "title": "Food Waste Analysis",
    "section": "References",
    "text": "References\n\n“From Farm to Kitchen: The Environmental Impacts of U.S. Food Waste.” EPA, Environmental Protection Agency, https://www.epa.gov/land-research/farm-kitchen-environmental-impacts-us-food-waste.\nEnvironmental Protection Agency. (n.d.). Technical Methodology for the Excess Food Opportunities Map. EPA. Retrieved December 9, 2022, from https://www.epa.gov/sustainable-management-food/technical-methodology-excess-food-opportunities-map\nEnvironmental Protection Agency. (n.d.). About the U.S. EPA Excess Food Opportunities Map. EPA. Retrieved December 9, 2022, from https://www.epa.gov/sustainable-management-food/excess-food-opportunities-map#whatis\nBureau, U. S. C. (n.d.). American Community Survey DP03SELECTED ECONOMIC CHARACTERISTICS. Explore census data. Retrieved December 9, 2022, from https://data.census.gov/table?q=DP03&t=Earnings%2B%28Individuals%29%3AIncome%2B%28Households%2C%2BFamilies%2C%2BIndividuals%29%3AIncome%2Band%2BEarnings&g=0100000US_0400000US06%240500000&y=2020&tid=ACSDP5Y2020.DP03&tp=false\nCalifornia, S. of. (n.d.). California Regions. CA Census. Retrieved December 9, 2022, from https://census.ca.gov/regions/"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Portfolio",
    "section": "",
    "text": "Stastistical Analysis: Food Waste and Average Household Income in California\n\n\n\nMEDS\n\n\nR\n\n\nStatistics\n\n\nFood Waste\n\n\nCalifornia\n\n\n\nResearching if there is a relationship between the amount of food waste produced and the average household income in California\n\n\n\nRuth Enriquez\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nMapping Out Optimal EEZs for Oyster Aquaculutre\n\n\n\nMEDS\n\n\nR\n\n\nGeospatial\n\n\nCalifornia\n\n\n\nIdentifying which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters\n\n\n\nRuth Enriquez\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-12-03-geospatial/geospatial_oysters.html#data",
    "href": "posts/2022-12-03-geospatial/geospatial_oysters.html#data",
    "title": "Mapping Out Optimal EEZs for Oyster Aquaculutre",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature\nI will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean I will use the General Bathymetric Chart of the Oceans (GEBCO).3\n\n\nExclusive Economic Zones\nI will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org."
  },
  {
    "objectID": "posts/2022-12-03-geospatial/geospatial_oysters.html#getting-started",
    "href": "posts/2022-12-03-geospatial/geospatial_oysters.html#getting-started",
    "title": "Mapping Out Optimal EEZs for Oyster Aquaculutre",
    "section": "Getting Started",
    "text": "Getting Started\nI will begin by loading all of the necessary packages to analyze the data. Then I will load and validate the data to ensure that it is all on the same coordinate reference system.\n::: {.cell}\n\n```{.r .cell-code}\n#loading in packages\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(here)\nlibrary(janitor)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(terra)\nlibrary(sf)\nlibrary(stringr)\n\n# setting file path\nsetwd(here())\n```\n:::\n\nread in the shapefile for the West Coast EEZ (wc_regions_clean.shp)\n\n\nShow the code\n#Reading in the West Coast shapefile\nwcEEZ <- st_read(here(\"posts\"\n                      , \"2022-12-03-geospatial\"\n                      , \"data\"\n                      , \"wc_regions_clean.shp\"))\n\n\nread in SST rasters\n\naverage_annual_sst_2008.tif\n\naverage_annual_sst_2009.tif\n\naverage_annual_sst_2010.tif\n\naverage_annual_sst_2011.tif\n\naverage_annual_sst_2012.tif\n\ncombine SST rasters into a raster stack\n\n\nShow the code\n#SST rasters -> raster stack\n##using list file path from lab\nfilelist <- list.files(here(\"posts\"\n                            , \"2022-12-03-geospatial\"\n                            , \"data\"\n                            , \"sst\"), full.names = TRUE)\n\n#reading in and storing files as a raster stack\norgStackSST <- rast(filelist)\n\n\nread in bathymetry raster (depth.tif)\n\n\nShow the code\n#Read in bathymetry raster tif file\noceanDepth <- rast(here(\"posts\"\n                        , \"2022-12-03-geospatial\"\n                        , \"data\"\n                        , \"depth.tif\"))\n\n\ncheck that data are in the same coordinate reference system\n\n\nShow the code\n#use st_crs to check the crs of each raster\nst_crs(wcEEZ) #WGS 84; EPSG 4326\nst_crs(orgStackSST) #WGS 84; EPSG 9001/9122\nst_crs(oceanDepth) #WGS 84, EPSG 4326\n\n\n\nreproject any data not in the same projection\n\n\n\n\nShow the code\n#reproject stackSST data to be the same as wcEEZ and oceanDepth\nstackSST <- project(orgStackSST, wcEEZ)\n\n#check that crs was successfully changed\nst_crs(stackSST)"
  },
  {
    "objectID": "posts/2022-12-03-geospatial/geospatial_oysters.html#getting-into-the-data",
    "href": "posts/2022-12-03-geospatial/geospatial_oysters.html#getting-into-the-data",
    "title": "Mapping Out Optimal EEZs for Oyster Aquaculutre",
    "section": "Getting into the Data",
    "text": "Getting into the Data\nNext, I need to process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. I don’t want to change the underlying depth data, so I will need to resample to match the SST data using the nearest neighbor approach.\n\nfind the mean SST from 2008-2012\n\n\nShow the code\n#find the mean of stackSST\nstackSSTmean <- mean(stackSST)\n\n#check that it calculated\nprint(stackSSTmean)\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :     mean \nmin value   : 281.3675 \nmax value   : 301.1837 \n\n\nconvert SST data from Kelvin to Celsius\n\n\nShow the code\n#converting stackSSTmean data from Kelvin to Celsius\nSSTcelsius <- stackSSTmean - 273.15\n\n\ncrop depth raster to match the extent of the SST raster\n\n\nShow the code\n#cropping the oceanDepth raster to match SST raster\noceanDepthCrop <- crop(oceanDepth, SSTcelsius)\n\n\nnote: the resolutions of the SST and depth data do not match\n\n\nresample the depth data to match the resolution of the SST data using the nearest neighbor approach\n\n\n\n\nShow the code\noceanDepthRe <- resample(oceanDepthCrop, SSTcelsius, method = 'near')\n\n\ncheck that the depth and SST match in resolution, extent, and coordinate reference system\n\n\n\n\nShow the code\n#checking that resolution, extent, and crs match\nprint(SSTcelsius)\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :      mean \nmin value   :  8.217548 \nmax value   : 28.033722 \n\n\nShow the code\nprint(oceanDepthRe)\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : depth \nmin value   : -5468 \nmax value   :  4218 \n\n\nShow the code\n#check that the resolution is the same\n#resolution written: x, y\n  #stackSSTmeanC\n    #resolution  : 0.04165905, 0.04165905\n  #oceanDepthRe\n    #resolution  : 0.04165905, 0.04165905\n\n#check that the extent is the same\n#extent written in: xmin, xmax, ymin, ymax\n  #stackSSTmeanC\n    #extent: -131.9848, -114.9879, 29.99208, 49.98842\n  #oceanDepthRe\n    #extent: -131.9848, -114.9879, 29.99208, 49.98842\n\n#check the crs is the same\n  #stackSSTmeanC\n    #coord. ref. : lon/lat WGS 84 (EPSG:4326)\n  #oceanDepthRe\n    #coord. ref. : lon/lat WGS 84 (EPSG:4326) \n\n\n\n\nShow the code\n#stacking the rasters to check if the dimensions are really the same\n#if they are NOT the same there will be an error/warning\ndepthSST <- c(SSTcelsius, oceanDepthRe)\n\n\n\nIdentifying Suitable Locations\nIn order to find suitable locations for marine aquaculture, I will need to find locations that are suitable in terms of both SST and depth.\n\nreclassify SST and depth data into locations that are suitable for oyster\n\n\n\nShow the code\n#identify suitable conditions for oysters\n#(taken from data given)...\n\n#sea surface temperature: 11-30°C\n#depth: 0-70 meters below sea level\n\n#reclassifying temp\ntemp <- c(-Inf, 11, NA,\n        11, 30, 1,\n        30, Inf, NA)\n\ntempMatrix <- matrix(temp, ncol = 3, byrow = TRUE)\n\nSSTreclass <- classify(SSTcelsius, rcl = tempMatrix, include.lowest = TRUE)\n\n#reclassifying depth\ndepth <- c(-Inf, -70, NA,\n           -70, 0, 1,\n           0, Inf, NA)\n\ndepthMatrix <- matrix(depth, ncol = 3, byrow = TRUE)\n\ndepthReclass <- classify(oceanDepthRe, rcl = depthMatrix, include.lowest = TRUE)\n\n\nfind locations that satisfy both SST and depth conditions\n\n\n\n\nShow the code\n#creating a function to combine the reclass\nfunC <- function(x, y) {\n  return(x*y)\n  }\n\n#finding locations that satisfy both SST and depth conditions\nsuitableLocations <-lapp(c(SSTreclass,depthReclass), fun = funC)\nplot(suitableLocations, col = \"blue\")\n\n\n\n\n\n\n\nDetermining the Most Suitable EEZ\nI want to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, we need to find the total area of suitable locations within each EEZ.\n\nselect suitable cells within West Coast EEZs\n\nfind area of grid cells\n\nfind the total suitable area within each EEZ\n\nfind the percentage of each zone that is suitable\n\n\n\n\nShow the code\n#setting up cell size\ncellSize <- cellSize(suitableLocations, mask = TRUE, unit = \"km\", transform = TRUE)\n\n#changing EEZ file to raster\nwcRaster <- rasterize(wcEEZ, suitableLocations, field =\"rgn\")\n\n#creating a mask, testing which variable combos work: wcRaster, suitableLocations, EEZcropSuitable\nwcMask <- mask(wcRaster, suitableLocations, inverse = FALSE, updatevalue = NA)\n\n#using zonal example from week 5 lab... applying zonal operations to help us find zones\nsuitableArea <- zonal(cellSize, wcMask, sum, na.rm = TRUE)\n\n#joining together data\nsuitableEEZ <- merge(wcEEZ, suitableArea, by = 'rgn') |> \n  mutate(suitable_area = area, percentage = (suitable_area/area_km2 * 100), .before = geometry)"
  },
  {
    "objectID": "posts/2022-12-03-geospatial/geospatial_oysters.html#visualizing-our-results",
    "href": "posts/2022-12-03-geospatial/geospatial_oysters.html#visualizing-our-results",
    "title": "Mapping Out Optimal EEZs for Oyster Aquaculutre",
    "section": "Visualizing our Results",
    "text": "Visualizing our Results\nNow that I have results, I need to present them!\nTime to create the following maps:\n\ntotal suitable area by region\n\npercent suitable area by region\n\n\n\n\nShow the code\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nShow the code\n#Creating a map for total suitable area by region\n\ntm_basemap(\"Esri.WorldStreetMap\")+\ntm_shape(suitableEEZ) +\n  tm_polygons(col = 'area',\n              palette = 'BrBG',\n              alpha = 0.5,\n              border.col = 'black') +\n  tm_layout( main.title = \"Suitable Areas for Oysters by Region: Total\",\n             main.title.position = 'center',\n             main.title.size = 0.5,\n             legend.outside = TRUE) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))+\n  tm_text(\"rgn\", size = 0.54)\n\n\n\n\n\n\n\n\n\nShow the code\n#Creating a map for percent suitable area by region\ntm_basemap(\"Esri.WorldStreetMap\")+\ntm_shape(suitableEEZ) +\n  tm_polygons(col = 'percentage',\n              palette = 'BrBG',\n              alpha = 0.5,\n              border.col = 'black') +\n  tm_layout( main.title = \"Suitable Areas for Oysters by Region: Percentage\",\n             main.title.position = 'center',\n             main.title.size = 0.5,\n             legend.outside = TRUE)+\n  tm_scale_bar(position = c(\"left\", 'bottom')) +\n  tm_text(\"rgn\", size = 0.54)"
  },
  {
    "objectID": "posts/2022-12-03-geospatial/geospatial_oysters.html#optimal-eezs-for-dungness-crab",
    "href": "posts/2022-12-03-geospatial/geospatial_oysters.html#optimal-eezs-for-dungness-crab",
    "title": "Mapping Out Optimal EEZs for Oyster Aquaculutre",
    "section": "Optimal EEZs for Dungness Crab",
    "text": "Optimal EEZs for Dungness Crab\nNow that I’ve worked through the solution for one group of species, I want to update my workflow to work for other species I am interested in. To do this I will create a function that would allow anyone to reproduce my results for other species. My function will be able to do the following:\n\n\naccept temperature and depth ranges and species name as inputs\n\ncreate maps of total suitable area and percent suitable area per EEZ with the species name in the title\n\n\nI am interested at looking for optimal EEZs for Dungness Crab. To get information on the depth and temperature requirements for Dungess Crab I will go to SeaLifeBase.\n\n\nShow the code\n# Creating the function\nsuitableMapFunction <- function(seaSurfaceTempLow, seaSurfaceTempHigh, oceanDepthLow, oceanDepthHigh, speciesName ){\n\nwcEEZ <- st_read(here(\"posts\"\n                      , \"2022-12-03-geospatial\"\n                      , \"data\"\n                      , \"wc_regions_clean.shp\"))\n\nfilelist <- list.files(here(\"posts\"\n                            , \"2022-12-03-geospatial\"\n                            , \"data\"\n                            , \"sst\"), full.names = TRUE)\n\norgStackSST <- rast(filelist)\n\noceanDepth <- rast(here(\"posts\"\n                        , \"2022-12-03-geospatial\"\n                        , \"data\"\n                        , \"depth.tif\"))\n\nstackSST <- project(orgStackSST, y =\"epsg:4326\")\n\nstackSSTmean <- mean(stackSST)\n\nSSTcelsius <- stackSSTmean - 273.15\n\noceanDepthCrop <- crop(oceanDepth, SSTcelsius)\n\noceanDepthRe <- resample(oceanDepthCrop, SSTcelsius, method = 'near')\n\ntemp <- c(-Inf, seaSurfaceTempLow, NA,\n        seaSurfaceTempLow, seaSurfaceTempHigh, 1,\n        seaSurfaceTempHigh, Inf, NA)\ntempMatrix <- matrix(temp, ncol = 3, byrow = TRUE)\nSSTreclass <- classify(SSTcelsius, rcl = tempMatrix, include.lowest = TRUE)\n\n\ndepth <- c(-Inf, oceanDepthLow, NA,\n           oceanDepthLow, oceanDepthHigh, 1,\n           oceanDepthHigh, Inf, NA)\ndepthMatrix <- matrix(depth, ncol = 3, byrow = TRUE)\ndepthReclass <- classify(oceanDepthRe, rcl = depthMatrix, include.lowest = TRUE)\n\nfunC <- function(x, y) {\n  return(x*y)\n  }\nsuitableLocations <-lapp(c(SSTreclass,depthReclass), fun = funC)\n\ncellSize <- cellSize(suitableLocations, mask = TRUE, unit = \"km\", transform = TRUE)\n\nwcRaster <- rasterize(wcEEZ, suitableLocations, field =\"rgn\")\n\nwcMask <- mask(wcRaster, suitableLocations, inverse = FALSE, updatevalue = NA)\n\nsuitableArea <- zonal(cellSize, wcMask, sum, na.rm = TRUE)\n\nsuitableEEZ <- merge(wcEEZ, suitableArea, by = 'rgn') |> \n  mutate(suitable_area = area, percentage = (suitable_area/area_km2 * 100), .before = geometry)\n\nsuitableAreaTotalMap <- tm_shape(suitableEEZ) +\n  tm_polygons(col = 'area',\n              palette = 'BrBG',\n              alpha = 0.5,\n              border.col = 'black') +\n  tm_layout(main.title = paste(\"Suitable Areas for\", speciesName, \"by Region: Total\"),\n             main.title.position = 'center',\n             title.size = 0.5,\n             legend.outside = TRUE) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))+\n  tm_text(\"rgn\", size = 0.54)\n\nsuitableAreaPercentMap <- tm_shape(suitableEEZ) +\n  tm_polygons(col = 'percentage',\n              palette = 'BrBG',\n              alpha = 0.5,\n              border.col = 'black') +\n  tm_layout(main.title = paste(\"Suitable Areas for\", speciesName, \" by Region: Percentage\"),\n            main.title.position = 'center',\n            title.size = 0.03,\n            legend.outside = TRUE)+\n  tm_scale_bar(position = c(\"left\", 'bottom')) +\n  tm_text(\"rgn\", size = 0.54)\n\ntmap_arrange(suitableAreaTotalMap, suitableAreaPercentMap, widths = c(.25, .75))\n}\n\n\n\n\nShow the code\n# Testing the function\ntestCrab <- suitableMapFunction(3, 19, 0, 360, \"Dungeness Crab\")\n\n\nReading layer `wc_regions_clean' from data source \n  `C:\\Users\\ruthe\\Documents\\ruthe808.github.io\\posts\\2022-12-03-geospatial\\data\\wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n\n\n\nShow the code\n# Seeing the results\ntestCrab"
  },
  {
    "objectID": "posts/2022-12-03-geospatial/geospatial_oysters.html",
    "href": "posts/2022-12-03-geospatial/geospatial_oysters.html",
    "title": "Mapping Out Optimal EEZs for Oyster Aquaculutre",
    "section": "",
    "text": "This work comes from a Geospatial Analyst & Remote Sensing homework assignment. We learned that aquaculture in marine environments holds the promise of being a significant contributor to the world’s food resources, offering a more sustainable protein alternative compared to traditional land-based meat farming.\nFor this work, I will be identifying which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level\n\nObjectives:\n\ncombining vector/raster data\nresampling raster data\nmasking raster data\nmap algebra"
  }
]